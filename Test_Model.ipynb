{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model with Images From Mapillary Vistas Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "# Data directory. Change this to download to a different directory, e.g. to an external drive to save space. \n",
    "# You need 20 GB to store all data.\n",
    "# If you use Google Colab to run this notebook, then you may want to point this to a Google Drive directory shared\n",
    "# between you and your assignment partner.\n",
    "dir_data = os.path.abspath(\"data\")\n",
    "\n",
    "# URLs to retrieve ground truth and images data from. \n",
    "url_truth = 'https://filesender.surf.nl/download.php?token=8aa09d58-7ba4-48c8-9955-ad1a864fb498&files_ids=13231626'\n",
    "dir_truth = os.path.join(dir_data, \"visastruth\")\n",
    "\n",
    "url_input = 'https://filesender.surf.nl/download.php?token=8aa09d58-7ba4-48c8-9955-ad1a864fb498&files_ids=13231628'\n",
    "dir_input = os.path.join(dir_data, \"viasas\")\n",
    "\n",
    "# Download and extraction function\n",
    "def download_extract(url: str):\n",
    "    # Create a temp directory to download into\n",
    "    with tempfile.TemporaryDirectory(dir=dir_data, prefix=\"download_\") as dir_temp:\n",
    "        print(f'Downloading: {url}')\n",
    "        zip_path = os.path.join(dir_temp, 'download.zip')\n",
    "        urlretrieve(url, zip_path, lambda n, size, total: sys.stdout.write(f'\\rProgress: {n*size/total*100:.2f} %'))\n",
    "        sys.stdout.write('\\n')\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        print(f'Unpacking archive.')\n",
    "        shutil.unpack_archive(zip_path, dir_data)\n",
    "\n",
    "# Create the data directory (if it does not exist)\n",
    "os.makedirs(dir_data, exist_ok=True)\n",
    "\n",
    "# Check if both the ground truth and input directories have been downloaded and extracted\n",
    "for dir, url in [(dir_truth, url_truth), (dir_input, url_input)]:\n",
    "    if not os.path.isdir(dir):\n",
    "        # Download the required files\n",
    "        print(f'Directory does not exist: {dir}')\n",
    "        download_extract(url)\n",
    "    else:\n",
    "        print(f'Directory already downloaded: {dir}')\n",
    "\n",
    "# Done!\n",
    "print(f'All data downloaded')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Target size of each sample in the dataset\n",
    "sample_size = (256, 128)\n",
    "\n",
    "# Directories for preprocessed datasets\n",
    "dir_truth_pp, dir_input_pp = (f'{d}_{sample_size[0]}_{sample_size[1]}' for d in (dir_truth, dir_input))\n",
    "\n",
    "# Run preprocessing\n",
    "for dir_full, dir_pp in ((dir_truth, dir_truth_pp), (dir_input, dir_input_pp)):\n",
    "    # Check if the directory already exists\n",
    "    if os.path.isdir(dir_pp):\n",
    "        print(f'Preprocessed directory already exists: {dir_pp}')\n",
    "        continue\n",
    "\n",
    "    print(f'Preprocessing: {dir_full}')\n",
    "\n",
    "    # Walk though the directory and preprocess each file \n",
    "    for root,_,files in  os.walk( dir_full ):\n",
    "        if len(files) == 0:\n",
    "            continue\n",
    "\n",
    "        print(f'Preprocessing sub-directory: {root.replace(dir_full, \"\")}')\n",
    "\n",
    "        # Create the directory in the preprocessed set\n",
    "        root_pp = root.replace(dir_full, dir_pp)\n",
    "        os.makedirs(root_pp, exist_ok=True)\n",
    "\n",
    "        for f in files:\n",
    "            if not f.endswith('.png'):\n",
    "                continue\n",
    "\n",
    "            # Resize and save PNG image\n",
    "            path_original = os.path.join(root,f)\n",
    "            img_resized = Image.open(path_original).resize(sample_size, Image.NEAREST)\n",
    "            img_resized.save(path_original.replace(dir_full, dir_pp), 'png', quality=100)\n",
    "\n",
    "print(f'Preprocessing done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Tuple\n",
    "import re\n",
    "\n",
    "# Each sample we downloaded can be identified by the name of the city as well as a frame and sequence id\n",
    "@dataclass\n",
    "class CityscapesSample:\n",
    "    city: str\n",
    "    seq_id: str\n",
    "\n",
    "    @property\n",
    "    def id(self):\n",
    "        return os.path.join(self.city, \"_\".join([self.city, self.seq_id]))\n",
    "\n",
    "    @staticmethod\n",
    "    def from_filename(filename: str):\n",
    "        # Create a CityscapesSample from a filename, which has a fixed structure {city}_{sequence}_{frame}\n",
    "        match = re.match(r\"^(\\w+)_(\\d+).*g$\", filename, re.I)\n",
    "        return CityscapesSample(match.group(1), match.group(2))\n",
    "\n",
    "\n",
    "# Each class that we aim to detect is assigned a name, id and color.\n",
    "@dataclass\n",
    "class CityscapesClass:\n",
    "    name: str       # The identifier of this label, e.g. 'car', 'person', ... .\n",
    "                    # We use them to uniquely name a class\n",
    "\n",
    "    ID: int         # An integer ID that is associated with this label.\n",
    "                    # The IDs are used to represent the label in ground truth images\n",
    "                    # An ID of -1 means that this label does not have an ID and thus\n",
    "                    # is ignored when creating ground truth images (e.g. license plate).\n",
    "                    # Do not modify these IDs, since exactly these IDs are expected by the\n",
    "                    # evaluation server.\n",
    "\n",
    "    trainId: int    # Feel free to modify these IDs as suitable for your method. Then create\n",
    "                    # ground truth images with train IDs, using the tools provided in the\n",
    "                    # 'preparation' folder. However, make sure to validate or submit results\n",
    "                    # to our evaluation server using the regular IDs above!\n",
    "                    # For trainIds, multiple labels might have the same ID. Then, these labels\n",
    "                    # are mapped to the same class in the ground truth images. For the inverse\n",
    "                    # mapping, we use the label that is defined first in the list below.\n",
    "                    # For example, mapping all void-type classes to the same ID in training,\n",
    "                    # might make sense for some approaches.\n",
    "                    # Max value is 255!\n",
    "\n",
    "    category: str   # The name of the category that this label belongs to\n",
    "\n",
    "    categoryId: int # The ID of this category. Used to create ground truth images\n",
    "                    # on category level.\n",
    "\n",
    "    hasInstances: bool # Whether this label distinguishes between single instances or not\n",
    "\n",
    "    ignoreInEval: bool # Whether pixels having this class as ground truth label are ignored\n",
    "                       # during evaluations or not\n",
    "\n",
    "    color: Tuple[int, int, int]       # The color of this label\n",
    "\n",
    "\n",
    "# List of classes that we want to detect in the input\n",
    "classes = [\n",
    "    #                 name                     ID    trainId   category            catId     hasInstances   ignoreInEval   color\n",
    "    CityscapesClass(  'unlabeled'            ,  0 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    CityscapesClass(  'ego vehicle'          ,  1 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    CityscapesClass(  'rectification border' ,  2 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    CityscapesClass(  'out of roi'           ,  3 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    CityscapesClass(  'static'               ,  4 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    CityscapesClass(  'dynamic'              ,  5 ,      255 , 'void'            , 0       , False        , True         , (111, 74,  0) ),\n",
    "    CityscapesClass(  'ground'               ,  6 ,      255 , 'void'            , 0       , False        , True         , ( 81,  0, 81) ),\n",
    "    CityscapesClass(  'road'                 ,  7 ,        0 , 'flat'            , 1       , False        , False        , (128, 64,128) ),\n",
    "    CityscapesClass(  'sidewalk'             ,  8 ,        1 , 'flat'            , 1       , False        , False        , (244, 35,232) ),\n",
    "    CityscapesClass(  'parking'              ,  9 ,      255 , 'flat'            , 1       , False        , True         , (250,170,160) ),\n",
    "    CityscapesClass(  'rail track'           , 10 ,      255 , 'flat'            , 1       , False        , True         , (230,150,140) ),\n",
    "    CityscapesClass(  'building'             , 11 ,        2 , 'construction'    , 2       , False        , False        , ( 70, 70, 70) ),\n",
    "    CityscapesClass(  'wall'                 , 12 ,        3 , 'construction'    , 2       , False        , False        , (102,102,156) ),\n",
    "    CityscapesClass(  'fence'                , 13 ,        4 , 'construction'    , 2       , False        , False        , (190,153,153) ),\n",
    "    CityscapesClass(  'guard rail'           , 14 ,      255 , 'construction'    , 2       , False        , True         , (180,165,180) ),\n",
    "    CityscapesClass(  'bridge'               , 15 ,      255 , 'construction'    , 2       , False        , True         , (150,100,100) ),\n",
    "    CityscapesClass(  'tunnel'               , 16 ,      255 , 'construction'    , 2       , False        , True         , (150,120, 90) ),\n",
    "    CityscapesClass(  'pole'                 , 17 ,        5 , 'object'          , 3       , False        , False        , (153,153,153) ),\n",
    "    CityscapesClass(  'polegroup'            , 18 ,      255 , 'object'          , 3       , False        , True         , (0  ,  0,  0) ),\n",
    "    CityscapesClass(  'traffic light'        , 19 ,        6 , 'object'          , 3       , False        , False        , (250,170, 30) ),\n",
    "    CityscapesClass(  'traffic sign'         , 20 ,        7 , 'object'          , 3       , False        , False        , (220,220,  0) ),\n",
    "    CityscapesClass(  'vegetation'           , 21 ,        8 , 'nature'          , 4       , False        , False        , (107,142, 35) ),\n",
    "    CityscapesClass(  'terrain'              , 22 ,        9 , 'nature'          , 4       , False        , False        , (152,251,152) ),\n",
    "    CityscapesClass(  'sky'                  , 23 ,       10 , 'sky'             , 5       , False        , False        , ( 70,130,180) ),\n",
    "    CityscapesClass(  'person'               , 24 ,       11 , 'human'           , 6       , True         , False        , (220, 20, 60) ),\n",
    "    CityscapesClass(  'rider'                , 25 ,       12 , 'human'           , 6       , True         , False        , (255,  0,  0) ),\n",
    "    CityscapesClass(  'car'                  , 26 ,       13 , 'vehicle'         , 7       , True         , False        , (  0,  0,142) ),\n",
    "    CityscapesClass(  'truck'                , 27 ,       14 , 'vehicle'         , 7       , True         , False        , (  0,  0, 70) ),\n",
    "    CityscapesClass(  'bus'                  , 28 ,       15 , 'vehicle'         , 7       , True         , False        , (  0, 60,100) ),\n",
    "    CityscapesClass(  'caravan'              , 29 ,      255 , 'vehicle'         , 7       , True         , True         , (  0,  0, 90) ),\n",
    "    CityscapesClass(  'trailer'              , 30 ,      255 , 'vehicle'         , 7       , True         , True         , (  0,  0,110) ),\n",
    "    CityscapesClass(  'train'                , 31 ,       16 , 'vehicle'         , 7       , True         , False        , (  0, 80,100) ),\n",
    "    CityscapesClass(  'motorcycle'           , 32 ,       17 , 'vehicle'         , 7       , True         , False        , (  0,  0,230) ),\n",
    "    CityscapesClass(  'bicycle'              , 33 ,       18 , 'vehicle'         , 7       , True         , False        , (119, 11, 32) ),\n",
    "    CityscapesClass(  'license plate'        , -1 ,      255 , 'vehicle'         , 7       , False        , True         , (0  ,0  ,  0) ),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from typing import Dict, Optional, Tuple, List\n",
    "import random\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "class CityscapesDataset(Dataset):\n",
    "    # Regular expression matching each PNG file in the dataset\n",
    "    __read_reg = r\"^(\\w+)_(\\d+)_(\\d+).*.png$\"\n",
    "\n",
    "    def __init__(self, dir_input: str, dir_truth: str, sample_size: Tuple[int,int], classes: List[CityscapesSample]):\n",
    "        super().__init__()\n",
    "\n",
    "        # These variables are also available as globals, but it is good practice to make classes\n",
    "        # not depend on global variables.\n",
    "        self.dir_input = dir_input\n",
    "        self.dir_truth = dir_truth\n",
    "        self.sample_size = sample_size\n",
    "        self.classes = classes\n",
    "\n",
    "        # Walk through the inputs directory and add each file to our items list\n",
    "        self.items = []\n",
    "        for (_, _, filenames) in os.walk(self.dir_input):\n",
    "            self.items.extend([CityscapesSample.from_filename(f) for f in filenames])\n",
    "\n",
    "        # Sanity check: do the provided directories contain any samples?\n",
    "        assert len(self.items) > 0, f\"No items found in {self.dir_input}\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, i: int) -> (torch.Tensor, torch.Tensor):\n",
    "        sample = self.items[i]\n",
    "\n",
    "        input = self.load_input(sample)\n",
    "        truth = self.load_truth(sample)\n",
    "\n",
    "        return self.transform(input, truth)\n",
    "\n",
    "    def load_input(self, sample: CityscapesSample) -> Image:\n",
    "        path = os.path.join(self.dir_input, f'{sample.id}.jpg')\n",
    "        return Image.open(path).convert(\"RGB\").resize(self.sample_size, Image.NEAREST)\n",
    "\n",
    "    def load_truth(self, sample:CityscapesSample) -> Image:\n",
    "        path = os.path.join(self.dir_truth, f'{sample.id}.png')\n",
    "        return Image.open(path).convert(\"RGB\").resize(self.sample_size, Image.NEAREST)\n",
    "\n",
    "    def transform(self, img: Image.Image, mask: Optional[Image.Image]) -> (torch.Tensor, torch.Tensor):\n",
    "        ## EXERCISE #####################################################################\n",
    "        #\n",
    "        # Data augmentation is a way to improve the accuracy of a model.\n",
    "        #\n",
    "        # Once you have a model that works, you can implement some data augmentation \n",
    "        # techniques here to further improve performance.\n",
    "        #\n",
    "        ##################################################################################\n",
    "        if self.dir_input == os.path.join(dir_input_pp, 'train'):\n",
    "            # Horizontal flip\n",
    "            if random.random() <= 0.3:\n",
    "                img = torchvision.transforms.RandomHorizontalFlip(p=1)(img)\n",
    "                mask = torchvision.transforms.RandomHorizontalFlip(p=1)(mask)\n",
    "            \n",
    "            \n",
    "            # Brightness adjustment\n",
    "            if random.random() <= 0.3:\n",
    "                img = torchvision.transforms.functional.adjust_brightness(img, random.random() + 0.5)\n",
    "\n",
    "            # Colour dithering\n",
    "            if random.random() <= 0.3:\n",
    "                color_jitter = torchvision.transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)\n",
    "                img = color_jitter(img)\n",
    "        ################################################################################# \n",
    "        # Convert the image to a tensor\n",
    "        \n",
    "        transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "        #                     std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        img = transform(img)\n",
    "        # If no mask is provided, then return only the image\n",
    "        if mask is None:\n",
    "            return img, None\n",
    "\n",
    "        # Transform the mask from an image with RGB-colors to an 1-channel image with the index of the class as value\n",
    "        mask_size = [s for s in self.sample_size]\n",
    "        mask = torch.from_numpy(np.array(mask)).permute((2,0,1))\n",
    "        target = torch.zeros((mask_size[1], mask_size[0]), dtype=torch.uint8)\n",
    "        for i,c in enumerate(classes):\n",
    "            eq = mask[0].eq(c.color[0]) & mask[1].eq(c.color[1]) & mask[2].eq(c.color[2])\n",
    "            target[eq] = c.trainId    \n",
    "            \n",
    "        return img, target\n",
    "\n",
    "    def masks_to_indices(self, masks: torch.Tensor) -> torch.Tensor:\n",
    "        _, indices = masks.softmax(dim=1).max(dim=1)\n",
    "        return indices\n",
    "\n",
    "    def to_image(self, indices: torch.Tensor) -> Image.Image:\n",
    "        target = torch.zeros((3, indices.shape[0], indices.shape[1]),\n",
    "                             dtype=torch.uint8, device=indices.device, requires_grad=False)\n",
    "\n",
    "        for i, lbl in enumerate(self.classes):\n",
    "            eq = indices.eq(lbl.trainId)\n",
    "\n",
    "            target[0][eq] = lbl.color[0]\n",
    "            target[1][eq] = lbl.color[1]\n",
    "            target[2][eq] = lbl.color[2]\n",
    "\n",
    "        return TF.to_pil_image(target.cpu(), 'RGB')\n",
    "\n",
    "# Create one instance of the CityscapesDataset for each split type\n",
    "ds_split = {\n",
    "    name:CityscapesDataset(os.path.join(dir_input_pp, name), os.path.join(dir_truth_pp, name), sample_size, classes)\n",
    "    for name in ([\"train\"])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "from io import BytesIO\n",
    "from base64 import b64encode\n",
    "\n",
    "import random\n",
    "\n",
    "# HTML templates for displaying random samples in a table\n",
    "template_table = '<table><thead><tr><th>Subset</th><th>Amount</th><th>Size</th><th>Input sample</th><th>Truth sample</th></tr></thead><tbody>{0}</tbody></table>'\n",
    "template_row = '<tr><td>{0}</td><td>{1}</td><td>{2}</td><td>{3}</td><td>{4}</td></tr>'\n",
    "template_img = '<img src=\"data:image/png;base64,{0}\"/>'\n",
    "\n",
    "# Display a random sample of each split of the dataset\n",
    "rows = []\n",
    "for name, ds_sub in ds_split.items():\n",
    "    # Draw a random sample from the dataset so that we can convert it back to an image\n",
    "    input, truth = random.choice(ds_sub)\n",
    "    #print(torch.unique(truth))\n",
    "\n",
    "    input = TF.to_pil_image(input)\n",
    "    truth = ds_sub.to_image(truth)\n",
    "\n",
    "    # Create a buffer to save each retrieved image into such that we can base64-encode it for diplay in our HTML table\n",
    "    with BytesIO() as buffer_input, BytesIO() as buffer_truth:\n",
    "        input.save(buffer_input, format='png')\n",
    "        truth.save(buffer_truth, format='png')\n",
    "\n",
    "        # Store one row of the dataset\n",
    "        images = [template_img.format(b64encode(b.getvalue()).decode('utf-8')) for b in (buffer_input, buffer_truth)]\n",
    "        rows.append(template_row.format(name, len(ds_sub), '&times;'.join([str(s) for s in input.size]), *images))\n",
    "\n",
    "# Render HTML table\n",
    "table = template_table.format(''.join(rows))\n",
    "display(HTML(table))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import resnet152\n",
    "\n",
    "class SeparableConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, bias=False):\n",
    "        super(SeparableConv2d, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size, stride, padding, dilation, groups=in_channels, bias=bias)\n",
    "        self.pointwise = nn.Conv2d(in_channels, out_channels, 1, 1, 0, 1, 1, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pointwise(x)\n",
    "        return x\n",
    "\n",
    "class ASPPConv(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, dilation, separable=False):\n",
    "        if separable:\n",
    "            conv_layer1 = SeparableConv2d(in_channels, out_channels, 3, padding=dilation, dilation=dilation, bias=False)\n",
    "            conv_layer2 = SeparableConv2d(out_channels, out_channels, 3, padding=dilation, dilation=dilation, bias=False)\n",
    "        else:\n",
    "            conv_layer1 = nn.Conv2d(in_channels, out_channels, 3, padding=dilation, dilation=dilation, bias=False)\n",
    "            conv_layer2 = nn.Conv2d(out_channels, out_channels, 3, padding=dilation, dilation=dilation, bias=False)\n",
    "\n",
    "        modules = [\n",
    "            conv_layer1,\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            conv_layer2,\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "        super(ASPPConv, self).__init__(*modules)\n",
    "\n",
    "\n",
    "class ASPPPooling(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ASPPPooling, self).__init__()\n",
    "        self.pool = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.pool(x)\n",
    "\n",
    "class ImprovedASPP(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, atrous_rates_list, separable=False):\n",
    "        super(ImprovedASPP, self).__init__()\n",
    "        self.aspp_modules = nn.ModuleList()\n",
    "\n",
    "        for atrous_rates in atrous_rates_list:\n",
    "            modules = []\n",
    "            modules.append(nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True)))\n",
    "\n",
    "            for rate in atrous_rates:\n",
    "                modules.append(ASPPConv(in_channels, out_channels, rate, separable))\n",
    "\n",
    "            modules.append(ASPPPooling(in_channels, out_channels))\n",
    "            aspp_module = nn.ModuleList(modules)\n",
    "            self.aspp_modules.append(aspp_module)\n",
    "\n",
    "        self.project = nn.Sequential(\n",
    "            nn.Conv2d(len(atrous_rates_list) * (len(atrous_rates_list[0]) + 2) * out_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = []\n",
    "        for aspp_module in self.aspp_modules:\n",
    "            res_per_module = []\n",
    "            for aspp in aspp_module:\n",
    "                res_per_module.append(aspp(x))\n",
    "\n",
    "            # Upsample ASPPPooling result before concatenating\n",
    "            res_per_module[-1] = F.interpolate(res_per_module[-1], size=res_per_module[-2].size()[2:], mode='bilinear', align_corners=True)\n",
    "            \n",
    "            if res_per_module:\n",
    "                res.append(torch.cat(res_per_module, dim=1))\n",
    "            \n",
    "\n",
    "        res = torch.cat(res, dim=1)\n",
    "        return self.project(res)\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super(Model, self).__init__()\n",
    "        self.backbone = resnet152(pretrained=True)\n",
    "        self.aspp = ImprovedASPP(2048, 256, atrous_rates_list=[(6, 12, 18), (12, 18, 24)])\n",
    "        self.decoder = DeepLabPlusDecoder(256, 20)\n",
    "        \n",
    "        self.backbone.layer4[0].conv2.stride = (1, 1)\n",
    "        self.backbone.layer4[0].downsample[0].stride = (1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone.conv1(x)\n",
    "        x = self.backbone.bn1(x)\n",
    "        x = self.backbone.relu(x)\n",
    "        x = self.backbone.maxpool(x)\n",
    "\n",
    "        x = self.backbone.layer1(x)\n",
    "        low_level_features = x\n",
    "        x = self.backbone.layer2(x)\n",
    "        x = self.backbone.layer3(x)\n",
    "        x = self.backbone.layer4(x)\n",
    "\n",
    "        x = self.aspp(x)\n",
    "        x = self.decoder(x, low_level_features)\n",
    "\n",
    "        return x\n",
    "\n",
    "class DeepLabPlusDecoder(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(DeepLabPlusDecoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 48, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(48)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.last_conv = nn.Sequential(\n",
    "          nn.Conv2d(in_channels + 48, 256, kernel_size=3, padding=1, bias=False),\n",
    "          nn.BatchNorm2d(256),\n",
    "          nn.ReLU(inplace=True),\n",
    "          nn.Dropout(0.5),\n",
    "          nn.Conv2d(256, 256, kernel_size=3, padding=1, bias=False),\n",
    "          nn.BatchNorm2d(256),\n",
    "          nn.ReLU(inplace=True),\n",
    "          nn.Dropout(0.5),\n",
    "          nn.Conv2d(256, 256, kernel_size=3, padding=1, bias=False),\n",
    "          nn.BatchNorm2d(256),\n",
    "          nn.ReLU(inplace=True),\n",
    "          nn.Dropout(0.5),\n",
    "          nn.Conv2d(256, num_classes, kernel_size=1)\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, low_level_features):\n",
    "        low_level_features = self.conv1(low_level_features)\n",
    "        low_level_features = self.bn1(low_level_features)\n",
    "        low_level_features = self.relu(low_level_features)\n",
    "        \n",
    "        x = F.interpolate(x, size=low_level_features.size()[2:], mode='bilinear', align_corners=True)\n",
    "        x = torch.cat((x, low_level_features), dim=1)\n",
    "        x = self.last_conv(x)\n",
    "\n",
    "        # Add upsampling step to match the target tensor size\n",
    "        x = F.interpolate(x, scale_factor=4, mode='bilinear', align_corners=True)\n",
    "        \n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "from torch import nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.fcn_resnet50 = models.segmentation.fcn_resnet50(pretrained=True)\n",
    "        self.fcn_resnet50.classifier[4] = nn.Conv2d(512, 20, kernel_size=(1, 1), stride=(1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fcn_resnet50(x)['out']\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Draw a random sample\n",
    "input, truth = random.choice(ds_split[\"train\"])\n",
    "model = Model()\n",
    "# Push through our network\n",
    "model = model.cpu()\n",
    "output = model(input.unsqueeze(0))\n",
    "\n",
    "# Display the input, output and truth tensors\n",
    "template_table = '<table><thead><tr><th>Tensor</th><th>Shape</th><th>Image</th></tr></thead><tbody>{0}</tbody></table>'\n",
    "template_row = '<tr><td>{0}</td><td>{1}</td><td><img src=\"data:image/png;base64,{2}\"/></td></tr>'\n",
    "\n",
    "input_img = TF.to_pil_image(input)\n",
    "output_img = ds_split[\"train\"].to_image(ds_split[\"train\"].masks_to_indices(output).squeeze(0))\n",
    "truth_img = ds_split[\"train\"].to_image(truth)\n",
    "\n",
    "rows = []\n",
    "for name, tensor, img in [('Input', input, input_img), ('Output', output, output_img), ('Target', truth, truth_img)]:\n",
    "    with BytesIO() as b:\n",
    "        img.save(b, format='png')\n",
    "        rows.append(template_row.format(name, list(tensor.shape), b64encode(b.getvalue()).decode('utf-8')))\n",
    "\n",
    "# Render HTML table\n",
    "table = template_table.format(''.join(rows))\n",
    "display(HTML(table))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
